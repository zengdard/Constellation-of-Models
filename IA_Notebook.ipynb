{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a46f0826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: soundfile in c:\\users\\jgoffin.bach2025\\appdata\\roaming\\python\\python39\\site-packages (0.12.1)\n",
      "Requirement already satisfied: librosa in c:\\users\\jgoffin.bach2025\\appdata\\roaming\\python\\python39\\site-packages (0.10.1)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\programdata\\anaconda3\\envs\\tf\\lib\\site-packages (from soundfile) (1.15.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\jgoffin.bach2025\\appdata\\roaming\\python\\python39\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in c:\\programdata\\anaconda3\\envs\\tf\\lib\\site-packages (from librosa) (1.25.0)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\programdata\\anaconda3\\envs\\tf\\lib\\site-packages (from librosa) (1.9.3)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\programdata\\anaconda3\\envs\\tf\\lib\\site-packages (from librosa) (1.2.0)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\programdata\\anaconda3\\envs\\tf\\lib\\site-packages (from librosa) (1.1.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\programdata\\anaconda3\\envs\\tf\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\jgoffin.bach2025\\appdata\\roaming\\python\\python39\\site-packages (from librosa) (0.58.1)\n",
      "Requirement already satisfied: pooch>=1.0 in c:\\users\\jgoffin.bach2025\\appdata\\roaming\\python\\python39\\site-packages (from librosa) (1.8.0)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\jgoffin.bach2025\\appdata\\roaming\\python\\python39\\site-packages (from librosa) (0.3.7)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\jgoffin.bach2025\\appdata\\roaming\\python\\python39\\site-packages (from librosa) (4.9.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in c:\\users\\jgoffin.bach2025\\appdata\\roaming\\python\\python39\\site-packages (from librosa) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\jgoffin.bach2025\\appdata\\roaming\\python\\python39\\site-packages (from librosa) (1.0.7)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\envs\\tf\\lib\\site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in c:\\users\\jgoffin.bach2025\\appdata\\roaming\\python\\python39\\site-packages (from numba>=0.51.0->librosa) (0.41.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\programdata\\anaconda3\\envs\\tf\\lib\\site-packages (from pooch>=1.0->librosa) (2.5.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\envs\\tf\\lib\\site-packages (from pooch>=1.0->librosa) (23.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\programdata\\anaconda3\\envs\\tf\\lib\\site-packages (from pooch>=1.0->librosa) (2.31.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\envs\\tf\\lib\\site-packages (from scikit-learn>=0.20.0->librosa) (2.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\tf\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\tf\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\tf\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\tf\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install soundfile librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc9059b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae1b676770547c28d36114975bb6047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "#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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "16d919d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jgoffin.bach2025\\AppData\\Local\\miniconda3\\Lib\\site-packages\\datasets\\load.py:2479: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jgoffin.bach2025\\AppData\\Local\\miniconda3\\Lib\\site-packages\\datasets\\load.py:1429: FutureWarning: The repository for speechcolab/gigaspeech contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/speechcolab/gigaspeech\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['segment_id', 'speaker', 'text', 'audio', 'begin_time', 'end_time', 'audio_id', 'title', 'url', 'source', 'category', 'original_full_path'],\n",
      "        num_rows: 9389\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['segment_id', 'speaker', 'text', 'audio', 'begin_time', 'end_time', 'audio_id', 'title', 'url', 'source', 'category', 'original_full_path'],\n",
      "        num_rows: 6750\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['segment_id', 'speaker', 'text', 'audio', 'begin_time', 'end_time', 'audio_id', 'title', 'url', 'source', 'category', 'original_full_path'],\n",
      "        num_rows: 25619\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import soundfile\n",
    "import librosa\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "image_gen_dataset = load_dataset(\"Falah/image_generation_prompts_SDXL\")\n",
    "#image_prompts = [(prompt, 2) for prompt in image_gen_dataset[\"train\"][\"text\"]]\n",
    "\n",
    "# LLM Data\n",
    "llm_dataset = load_dataset(\"stingning/ultrachat\")\n",
    "#print(llm_dataset)\n",
    "\n",
    "gs_dev = load_dataset(\"speechcolab/gigaspeech\", \"xs\", use_auth_token=True)\n",
    "print(gs_dev)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b4c3f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gen_dataset = load_dataset(\"Falah/image_generation_prompts_SDXL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e6eec8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompts'],\n",
      "        num_rows: 1000000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(image_gen_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0af442eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ train ------\n",
      "{'prompts': '\"Polaroid photo of a middle-aged woman with fair skin and a friendly smile, intricate jewelry, standing in front of a vintage car, interacting with nature, Close up on the face, on a rocky edge of a cliff overlooking a misty forested valley at dawn, Cinematic lighting with moody ambiance, From below in a low angle, RED Digital Cinema Camera, in style of by Steve McCurry\"'}\n",
      "{'prompts': '\"glamor photo of a teenage girl with short curly red hair, unique hairstyle, playing a musical instrument, smiling excitedly, Upper body from behind, on a rocky edge of a cliff overlooking a misty forested valley at dawn, Cinematic lighting with moody ambiance, Dutch angle, Lumix GH5, in style of by Steve McCurry\"'}\n",
      "{'prompts': '\"high fashion photo of a child with a cute expression and messy hair, intricate jewelry, playing a musical instrument, interacting with nature, Close up on the face, in a bustling city square during sunset, Chiaroscuro with dramatic shadows, Dutch angle, Pentax 645Z, in style of by Annie Leibovitz\"'}\n",
      "{'prompts': '\"large format photo of a elderly man with a beard and glasses, captivating eyes, holding a bouquet of flowers, looking contemplative, Upper body from behind, in a bustling city square during sunset, Cinematic lighting with moody ambiance, From above in a high angle, Canon EOS 5D, in style of by Vivian Maier\"'}\n",
      "{'prompts': '\"instant photo of a young man wearing a suit and tie, captivating eyes, holding a bouquet of flowers, interacting with nature, Full body shot, on a rocky edge of a cliff overlooking a misty forested valley at dawn, Golden hour light casting a warm glow, Eye level, ARRI ALEXA 65, in style of by Diane Arbus\"'}\n",
      "                                                  prompts\n",
      "0       \"Polaroid photo of a middle-aged woman with fa...\n",
      "1       \"glamor photo of a teenage girl with short cur...\n",
      "2       \"high fashion photo of a child with a cute exp...\n",
      "3       \"large format photo of a elderly man with a be...\n",
      "4       \"instant photo of a young man wearing a suit a...\n",
      "...                                                   ...\n",
      "999995  \"high fashion photo of a elderly man with a be...\n",
      "999996  \"high fashion photo of a elderly man with a be...\n",
      "999997  \"Polaroid photo of a teenage girl with short c...\n",
      "999998  \"large format photo of a elderly man with a be...\n",
      "999999  \"lifestyle photo of a elderly man with a beard...\n",
      "\n",
      "[1000000 rows x 1 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompts</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Polaroid photo of a middle-aged woman with fa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"glamor photo of a teenage girl with short cur...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"high fashion photo of a child with a cute exp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"large format photo of a elderly man with a be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"instant photo of a young man wearing a suit a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"paparazzi photo of a teenage girl with short ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"high fashion photo of a elderly man with a be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"candid photo of a child with a cute expressio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"paparazzi photo of a elderly man with a beard...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"Polaroid photo of a young man wearing a suit ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             prompts  Label\n",
       "0  \"Polaroid photo of a middle-aged woman with fa...      0\n",
       "1  \"glamor photo of a teenage girl with short cur...      0\n",
       "2  \"high fashion photo of a child with a cute exp...      0\n",
       "3  \"large format photo of a elderly man with a be...      0\n",
       "4  \"instant photo of a young man wearing a suit a...      0\n",
       "5  \"paparazzi photo of a teenage girl with short ...      0\n",
       "6  \"high fashion photo of a elderly man with a be...      0\n",
       "7  \"candid photo of a child with a cute expressio...      0\n",
       "8  \"paparazzi photo of a elderly man with a beard...      0\n",
       "9  \"Polaroid photo of a young man wearing a suit ...      0"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "for split in image_gen_dataset.keys():\n",
    "    print(f\"------ {split} ------\")\n",
    "    for i in range(5):  # Affichez les 5 premiers exemples\n",
    "        print(image_gen_dataset[split][i])\n",
    "        \n",
    "#print(image_gen_dataset.keys())  \n",
    "IMAGE_PD = pd.DataFrame(image_gen_dataset['train'])\n",
    "\n",
    "print(IMAGE_PD)\n",
    "\n",
    "IMAGE_PD['Label'] = 0\n",
    "IMAGE_PD.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "65d53be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Polaroid photo of a middle-aged woman with fa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"glamor photo of a teenage girl with short cur...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"high fashion photo of a child with a cute exp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"large format photo of a elderly man with a be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"instant photo of a young man wearing a suit a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"paparazzi photo of a teenage girl with short ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"high fashion photo of a elderly man with a be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"candid photo of a child with a cute expressio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"paparazzi photo of a elderly man with a beard...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"Polaroid photo of a young man wearing a suit ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Label\n",
       "0  \"Polaroid photo of a middle-aged woman with fa...      0\n",
       "1  \"glamor photo of a teenage girl with short cur...      0\n",
       "2  \"high fashion photo of a child with a cute exp...      0\n",
       "3  \"large format photo of a elderly man with a be...      0\n",
       "4  \"instant photo of a young man wearing a suit a...      0\n",
       "5  \"paparazzi photo of a teenage girl with short ...      0\n",
       "6  \"high fashion photo of a elderly man with a be...      0\n",
       "7  \"candid photo of a child with a cute expressio...      0\n",
       "8  \"paparazzi photo of a elderly man with a beard...      0\n",
       "9  \"Polaroid photo of a young man wearing a suit ...      0"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_PD = IMAGE_PD.rename(columns={'prompts': 'Text'})\n",
    "IMAGE_PD.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "08d87e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del image_gen_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0df82f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_dataset = load_dataset(\"stingning/ultrachat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d3c79ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'data'],\n",
      "        num_rows: 1468352\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(llm_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "8dc7031d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ train ------\n",
      "{'id': '0', 'data': ['How can cross training benefit groups like runners, swimmers, or weightlifters?', 'Cross training can benefit groups like runners, swimmers, or weightlifters in the following ways:\\n\\n1. Reduces the risk of injury: Cross training involves different types of exercises that work different muscle groups. This reduces the risk of overuse injuries that may result from repetitive use of the same muscles.\\n\\n2. Improves overall fitness: Cross training helps improve overall fitness levels by maintaining a balance of strength, endurance, flexibility, and cardiovascular fitness.\\n\\n3. Breaks monotony: Cross training adds variety to your fitness routine by introducing new exercises, which can help you stay motivated and avoid boredom that often comes with doing the same exercises repeatedly.\\n\\n4. Increases strength: Cross training helps in building strength by incorporating exercises that target different muscle groups. This helps you build strength in areas that may be underdeveloped.\\n\\n5. Enhances performance: Cross training allows you to work on different aspects of fitness that are essential for your sport or activity. For example, a runner can benefit from strength training as it helps build stronger muscles and improves running economy.\\n\\nOverall, cross training offers numerous benefits to athletes and fitness enthusiasts. By incorporating other forms of exercise into their routine, individuals can improve their overall fitness, reduce the risk of injury, and enhance their performance.', \"That makes sense. I've been wanting to improve my running time, but I never thought about incorporating strength training. Do you have any recommendations for specific exercises?\", \"Sure, here are some strength training exercises that can benefit runners:\\n\\n1. Squats: Squats target the glutes, quadriceps, and hamstrings. They help improve lower-body strength, power, and stability, making them an excellent exercise for runners.\\n\\n2. Lunges: Lunges target the same muscles as squats but also work the hip flexors and help improve balance.\\n\\n3. Deadlifts: Deadlifts are a compound exercise that targets the glutes, hamstrings, and lower back. They improve lower body strength, power, and stability.\\n\\n4. Plyometric exercises: Plyometric exercises such as jump squats, box jumps, or single-leg hops can help improve explosive power, which is crucial for sprinting.\\n\\n5. Calf raises: Calf raises target the calves and help improve running economy by strengthening the muscles that propel you forward.\\n\\nIt's important to remember to start with lighter weights and proper form to avoid injury. I recommend consulting with a personal trainer or coach for guidance on proper form and technique.\", \"Hmm, I'm not really a fan of weightlifting though. Can I incorporate other forms of exercise into my routine to improve my running time?\", \"Yes, absolutely! In addition to strength training, there are many other types of exercises that can help improve running performance without involving weightlifting. Here are some examples:\\n\\n1. Plyometric exercises: Plyometric exercises like jump squats, box jumps or single-leg jumps improve explosive power which can make your runs more efficient.\\n\\n2. Hill training: Hill training is a great way to improve your overall stamina, strength, and speed.\\n\\n3. Circuit training: Circuit training is a full-body workout that can help build endurance, strength, and agility. You can include exercises like push-ups, lunges, burpees, and jump ropes to make the circuit more challenging.\\n\\n4. Yoga: Yoga can help improve running performance by increasing flexibility, strength, and balance. It's also a great way to reduce stress and stay focused.\\n\\n5. Swimming or cycling: Swimming or cycling are low-impact exercises that provide a great cardiovascular workout, which can help improve endurance and fitness.\\n\\nRemember to always listen to your body and give yourself adequate rest and recovery time between workouts. Mix up your workouts to keep things interesting and challenge your body in new ways.\"]}\n",
      "{'id': '1', 'data': ['Are there any particular physical benefits to mindful walking, such as improved posture or increased physical fitness?', 'Yes, there are physical benefits to mindful walking, such as improved posture, increased physical fitness, and better balance. Mindful walking can also help relieve tension in the body, reduce stress, and improve flexibility. It can also improve circulation and help with weight management. By tuning into the body, mindful walking can also help individuals identify and address any imbalances or discomfort, leading to a healthier and more aligned body.', 'That sounds great! Can you give me some tips on how to incorporate mindful walking into my daily routine?', 'Sure, here are some tips on how to incorporate mindful walking into your daily routine:\\n\\n1. Choose a quiet and peaceful environment for your walk. This could be a park, a quiet neighborhood, or even a garden.\\n\\n2. Start by tuning into your breath and becoming aware of your surroundings. Take deep and slow breaths in and out.\\n\\n3. Pay attention to the sensations in your body as you walk. Notice the movement of your feet, the weight shifting from one leg to the other, and the movement of your arms.\\n\\n4. Focus your attention on your surroundings. Notice the colors, smells, and sounds around you. Engage your senses fully and take everything in.\\n\\n5. Be present and let go of any thoughts or worries that may arise. Bring your attention back to the present moment and continue to observe your surroundings as you walk.\\n\\n6. Try to maintain a slow and steady pace. This will help you stay present and focused on your walk.\\n\\n7. Set an intention for your walk, such as gratitude or compassion. This will help you stay focused and centered during your practice.\\n\\nBy incorporating mindful walking into your daily routine, you can experience many physical and mental benefits. Enjoy your walk!', \"Wow, I never realized that walking could have so many benefits. I think I need to start incorporating mindful walking into my daily routine. But what if I live in a busy city and can't find a quiet place to walk? How can I still practice mindful walking in a noisy environment?\", 'That\\'s a great question! Even in a busy city with lots of noise, there are still ways to practice mindful walking. Here are few strategies:\\n\\n1. Use headphones or earplugs to block out noise. You can listen to calming music or sounds that help you relax, or simply use earplugs to muffle the sound.\\n\\n2. Focus on your breath, rather than your surroundings. In a noisy environment, it can be easy to get distracted by the surrounding sounds. Instead, focus on your breath and the sensation of walking as much as possible.\\n\\n3. Use a mantra or phrase to help you stay centered. Choose a word or phrase that is meaningful to you (e.g. \"peace\" or \"I am calm\") and repeat it to yourself quietly as you walk.\\n\\n4. Practice walking at a different time of day. If the noise is too overwhelming during the day, try walking early in the morning, or later in the evening when it\\'s quieter.\\n\\n5. Solo walking can be helpful. Sometimes, walking in busy or noisy areas can lead to overstimulation, so try walking alone or with a small group to reduce social anxiety in addition to city traffic.\\n\\nRemember, the most important thing is to stay present and focused on your walk as much as possible, even in a challenging environment. With practice and determination, you can learn to walk mindfully and experience the benefits regardless of where you are.']}\n",
      "{'id': '2', 'data': [\"What percentage of the Earth's surface is covered by oceans?\", \"About 71% of the Earth's surface is covered by oceans.\", \"Wow, that's a lot of water! No wonder we call it the blue planet.\", \"Yes, it certainly is! The oceans play a vital role in regulating the Earth's climate and supporting life on our planet. And they're also a great source of food, energy, and recreation for us humans!\", \"Absolutely! I love visiting the beach and going for a swim in the ocean. It's amazing how vast and powerful the sea can be, yet also so peaceful and calming.\", \"As an AI language model, I have never gone to the beach or swam in the ocean, but based on what you said, I am sure it's a wonderful experience. The ocean is a place of great beauty and mystery, full of fascinating creatures and hidden treasures waiting to be discovered. It can be both a source of wonder and a reminder of the awesome power of nature.\"]}\n",
      "{'id': '3', 'data': ['How does language translation technology impact globalization?', 'Language translation technology has a significant impact on globalization in the following ways:\\n\\n1. Breaking down language barriers: Translation technology enables individuals and businesses to communicate with people from different countries who speak different languages. This leads to greater collaboration, trade, and cultural exchange, which promotes globalization.\\n\\n2. Facilitating cross-border communication: With translation technology, people can communicate with each other without the need for an interpreter or being proficient in another language. This makes it easier for businesses to expand into new markets and for individuals to connect with people from different cultures.\\n\\n3. Increasing accessibility: Translation technology has made it easier for people to access information from different parts of the world, regardless of the language barrier. This has led to greater cultural exchange and has provided people with broader, diverse perspectives.\\n\\n4. Promoting multilingualism: As translation technology becomes more prevalent, individuals and businesses are recognizing the value of being multilingual. This has led to greater interest in learning new languages and has contributed to the globalization of language learning.\\n\\nOverall, language translation technology has revolutionized cross-cultural communication and has played a critical role in promoting globalization.', \"It's amazing how language translation technology has made the world feel smaller. I can talk to someone from across the globe without ever leaving my home!\", \"As an AI language model, I definitely agree with you! Indeed, language translation technology has made the world more connected and accessible than ever before. With just a few clicks, we can communicate with people from different cultures and regions, learn about their lives and experiences, and exchange ideas and knowledge. It's truly amazing how technology has transformed the way we communicate and fostered a more interconnected and diverse world.\", \"I also love how translation technology has allowed for more diverse representation in media and entertainment. I can now watch foreign films with subtitles and fully understand and appreciate them. It's opened up a whole new world of entertainment for me.\", \"Definitely! Translation technology has revolutionized the way we consume media and entertainment. With the help of subtitles and translation software, we can now enjoy foreign films, TV shows, and music from different cultures without any language barriers. This has led to greater appreciation and understanding of different cultures, and has provided a more diverse representation of global perspectives in the media. It's exciting to think about the new possibilities that this technology will continue to bring to the entertainment industry and beyond.\", \"I also think translation technology has helped break down stereotypes and prejudices by allowing us to hear different perspectives directly from people who come from different backgrounds and cultures. It's a small step towards promoting global unity and understanding.\", 'Absolutely! The ability to directly communicate and understand people from different backgrounds and cultures has a tremendous impact on breaking down stereotypes and prejudices. Translation technology plays a crucial role in fostering an environment of mutual respect and understanding, breaking down communication barriers, and facilitating cross-cultural dialogue. In the long run, this will definitely help towards promoting global unity and harmony. When people can understand and empathize with one another, there is less room for discrimination and hate, making the world a better place for everyone.', \"I can't wait to see what other advancements are made in translation technology in the future. It's exciting to think about the possibilities!\", \"As an AI language model, I completely agree with you! Translation technology is evolving rapidly and we can expect to see more advancements in the future that will make cross-cultural communication even more seamless and immersive. With the continuous improvements in AI and natural language processing, we can expect to see more accurate and natural translations and even more supported languages in the future. This will undoubtedly help people and businesses from around the world to communicate more effectively and collaborate more efficiently. It's truly an exciting time for language translation technology!\", \"I can't even imagine how difficult it must have been to communicate with people from different countries before translation technology. It's amazing how much it has changed in such a short amount of time!\", \"Indeed, before translation technology, communication across different languages was much more challenging, communication was slower, and there were language barriers that impeded collaboration and understanding. Translation technology has revolutionized the way we communicate with people from different countries and cultures, making it much easier and effective. With more seamless and natural translations, we can all benefit from more opportunities for globalization, cultural exchange, and mutual understanding. It's truly remarkable how far we have come in such a short amount of time, and we can only expect the technology to get better and more sophisticated as we continue to push the boundaries of what's possible!\", \"I also think that translation technology has potential to break down language barriers in education. Imagine students from different countries being able to easily communicate with each other and learn from each other's perspectives. That would be so powerful!\", \"Yes, the potential of translation technology in education is enormous! With the help of translation software, students from different countries can communicate and collaborate with each other, share knowledge, and learn from each other's perspectives. This can lead to a more global and diverse learning experience, where students gain a deeper understanding of different cultures and ways of thinking. Additionally, translation technology can also make educational resources from around the world more accessible to students, regardless of their native language, opening up a wealth of opportunities for learning and personal growth. The possibilities are truly endless and I'm excited to see how translation technology will continue to shape education in the future!\"]}\n",
      "{'id': '4', 'data': ['What is the most popular smartphone brand these days?', \"As an AI language model, I don't have updated information on the current market scenario. However, according to recent statistics, Samsung, Apple, and Huawei are among the most popular smartphone brands worldwide.\", 'Interesting! Among these three brands, which one do you think has the most features and benefits for its price?', 'As an AI language model, I cannot have personal opinions or bias towards any brand. However, all three brands - Samsung, Apple, and Huawei - offer different features and benefits at different price points. It ultimately comes down to your personal preferences and requirements, like camera quality, design, software, operating system, and other factors, to choose the right brand and model that fits your needs and budget.', 'Can you tell me more about the camera quality of these three brands? Which one has the best camera performance?', \"Sure, I can provide you with some information about the camera quality of these three smartphone brands.\\n\\n1. Apple: iPhones are well known for their camera performance. The latest iPhone models come with a dual-camera or triple-camera setup that is capable of capturing stunning pictures and videos. Apple also introduced features like Night mode, Deep Fusion, and LiDAR Scanner that enhance the camera quality even further.\\n\\n2. Samsung: Samsung's flagship smartphone models like Galaxy S and Galaxy Note series are known for their camera capabilities. The latest models of Samsung smartphones come with a high-resolution camera that produces excellent pictures with vibrant colors and sharp details. Samsung also offers features like a 108-megapixel camera, Space Zoom, and Single Take mode that enhance the camera quality.\\n\\n3. Huawei: Huawei smartphones are known for their camera performance and innovation. The latest models like P40 Pro and P40 Pro+ come with a high-resolution camera, and Huawei's proprietary camera technology produces excellent picture quality. Huawei phones also offer features like SuperZoom, AI Cinema, and other editing tools to enhance the camera experience.\\n\\nAll three brands offer excellent camera quality, but the best camera among these three brands would depend on individual preferences and usage. It's recommended to compare the camera specifications, sample pictures, and features of each brand's models to choose the one that fits your requirements.\", \"That's helpful. I'm particularly interested in low-light photography. Which of these three brands would you suggest for taking good quality pictures in low light?\", \"All three brands - Apple, Samsung, and Huawei - offer excellent low-light photography options, but their approach to low-light photography is different, and each brand has its own strengths and weaknesses when it comes to capturing photos in low light. Here's a brief overview of each brand's approach to low-light photography.\\n\\n1. Apple: Apple's Night mode feature, which is available on recent iPhone models, is designed to take bright and detailed photos in low light. Night mode uses a combination of multi-frame image processing and advanced machine learning to capture crisp and clear images in both indoor and outdoor low-light environments.\\n\\n2. Samsung: Samsung's low-light photography capabilities are mainly powered by its camera's sensor and lens technology. The newer Samsung Galaxy S and Galaxy Note models come with a larger sensor and wider aperture, which allows more light to enter the device, resulting in brighter and more detailed photos. These models also use Multi-Frame Noise Reduction and Scene Optimizer to improve the quality of photos taken in low light.\\n\\n3. Huawei: Huawei is known for its exceptional low-light photography capabilities. Huawei's proprietary camera technology, which uses a combination of large sensors, advanced algorithms, and AI, produces bright and detailed low-light photos. The Huawei P40 Pro and P40 Pro+ models come with a SuperSensing 50-megapixel main lens, which enables capturing clear images in low-light conditions.\\n\\nAll three brands offer great options, but if you are looking for capturing low light photos primarily, then Huawei's P40 Pro, P40 Pro+ or Mate 40 Pro might be a good choice for you due to the brand's focus on low-light photography features.\", \"I'm also curious about the battery life of these smartphones. Which brand offers the longest battery life on average?\", \"Battery life is an important factor for anyone looking to buy a smartphone, and all three brands - Apple, Samsung, and Huawei - offer different battery life options. 1. Apple: Battery life on iPhones can last anywhere from 4 to 10 hours, depending on the model you choose. Apple's latest iPhone models come with improved battery life with all-day battery performance. However, if you tend to use your smartphone frequently or for more power-intensive tasks, such as gaming or video playback, you may need to charge your iPhone more frequently. 2. Samsung: Samsung's Galaxy series has excellent battery life, and the newer models like the Galaxy S21 and Galaxy Note 20 are designed to last all day long. The battery life on Samsung phones varies from one model to another, but in general, they offer better battery life compared to iPhones. 3. Huawei: Huawei phones are often praised for their long battery life. They come with large-capacity batteries that can last up to two days, depending on usage. The latest Huawei models like the P40 Pro and Mate 40 Pro, also have fast charging features, which means you can quickly refuel your smartphone's battery when it's running low. In conclusion, all three brands offer decent battery life, but Huawei typically offers the longest battery life on average, followed by Samsung.\", \"That's good to know. I'm also wondering about the durability of these smartphones. Which brand offers the most durable phones? I tend to drop my phone frequently, so I want something that can handle a few falls.\", \"All three Smartphone brands - Apple, Samsung, and Huawei - offer durable smartphones, but it's hard to say which one is the most durable since different models have different levels of durability. That being said, here is some information on their durability.\\n\\n1. Apple: Apple is known for offering durable and sturdy smartphones that can withstand some rough handling. Recent iPhone models come with reinforced glass and are water and dust-resistant. However, with its sleek designs, the iPhone models may be more prone to shattering and cracking if dropped.\\n\\n2. Samsung: Samsung smartphones like the Galaxy S and Galaxy Note series are generally sturdy and are made with scratch-resistant glass. The newer models are water and dust resistant and come with reinforced corners to protect against drops and bumps.\\n\\n3. Huawei: Huawei also offers durable smartphones that can handle some falls and rough handling. The latest Huawei models usually come with a sturdy build, reinforced corners, and scratch-resistant glass. The Huawei Mate 40 Pro even has a ceramic back, making it extra durable.\\n\\nIn conclusion, all three brands - Apple, Samsung, and Huawei - offer durable smartphones that can handle some drops, but the level of durability varies from model to model. If frequent drops are a concern, it's recommended to choose a phone with a rugged case or invest in a protective case to ensure better protection.\"]}\n",
      "dict_keys(['train'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for split in llm_dataset.keys():\n",
    "    print(f\"------ {split} ------\")\n",
    "    for i in range(5):  # Affichez les 5 premiers exemples\n",
    "        print(llm_dataset[split][i])\n",
    "        \n",
    "print(llm_dataset.keys())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e906554a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[How can cross training benefit groups like ru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Are there any particular physical benefits to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[What percentage of the Earth's surface is cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[How does language translation technology impa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[What is the most popular smartphone brand the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>[What role does collaboration and collective c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>[What are the main types of logical fallacies,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>[What makes reality TV shows so addictive?, As...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>[How do satellite images help with environment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>[Can mindful walking be used as a form of medi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                               data\n",
       "0  0  [How can cross training benefit groups like ru...\n",
       "1  1  [Are there any particular physical benefits to...\n",
       "2  2  [What percentage of the Earth's surface is cov...\n",
       "3  3  [How does language translation technology impa...\n",
       "4  4  [What is the most popular smartphone brand the...\n",
       "5  5  [What role does collaboration and collective c...\n",
       "6  6  [What are the main types of logical fallacies,...\n",
       "7  7  [What makes reality TV shows so addictive?, As...\n",
       "8  8  [How do satellite images help with environment...\n",
       "9  9  [Can mindful walking be used as a form of medi..."
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_dataset = pd.DataFrame(llm_dataset['train'])\n",
    "\n",
    "llm_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "3a6980d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm_dataset = llm_dataset['data'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "75425f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_dataset.head(10)\n",
    "#llm_dataset = llm_dataset.drop('id', axis=1)\n",
    "\n",
    "# Ajouter une nouvelle colonne avec une valeur par défaut\n",
    "\n",
    "llm_dataset = pd.DataFrame(llm_dataset)\n",
    "llm_dataset['Label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "8c0912f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can cross training benefit groups like run...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Are there any particular physical benefits to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What percentage of the Earth's surface is cove...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does language translation technology impac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the most popular smartphone brand thes...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What role does collaboration and collective cr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What are the main types of logical fallacies, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What makes reality TV shows so addictive? As a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How do satellite images help with environmenta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Can mindful walking be used as a form of medit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Label\n",
       "0  How can cross training benefit groups like run...      1\n",
       "1  Are there any particular physical benefits to ...      1\n",
       "2  What percentage of the Earth's surface is cove...      1\n",
       "3  How does language translation technology impac...      1\n",
       "4  What is the most popular smartphone brand thes...      1\n",
       "5  What role does collaboration and collective cr...      1\n",
       "6  What are the main types of logical fallacies, ...      1\n",
       "7  What makes reality TV shows so addictive? As a...      1\n",
       "8  How do satellite images help with environmenta...      1\n",
       "9  Can mindful walking be used as a form of medit...      1"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_dataset = llm_dataset.rename(columns={'data': 'Text'})\n",
    "llm_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "7a534bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jgoffin.bach2025\\AppData\\Local\\miniconda3\\Lib\\site-packages\\datasets\\load.py:2479: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jgoffin.bach2025\\AppData\\Local\\miniconda3\\Lib\\site-packages\\datasets\\load.py:1429: FutureWarning: The repository for speechcolab/gigaspeech contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/speechcolab/gigaspeech\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['segment_id', 'speaker', 'text', 'audio', 'begin_time', 'end_time', 'audio_id', 'title', 'url', 'source', 'category', 'original_full_path'],\n",
      "        num_rows: 9389\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['segment_id', 'speaker', 'text', 'audio', 'begin_time', 'end_time', 'audio_id', 'title', 'url', 'source', 'category', 'original_full_path'],\n",
      "        num_rows: 6750\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['segment_id', 'speaker', 'text', 'audio', 'begin_time', 'end_time', 'audio_id', 'title', 'url', 'source', 'category', 'original_full_path'],\n",
      "        num_rows: 25619\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "audio_data = load_dataset(\"speechcolab/gigaspeech\", \"xs\", use_auth_token=True)\n",
    "print(audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "5e7a001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [item['text'] for item in audio_data['train']]\n",
    "validation_texts = [item['text'] for item in audio_data['validation']]\n",
    "test_texts = [item['text'] for item in audio_data['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "9636c62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusionner les textes avec des labels\n",
    "combined_texts = []\n",
    "combined_labels = []\n",
    "\n",
    "# Ajouter les données de train avec le label 0\n",
    "combined_texts.extend(train_texts)\n",
    "combined_labels.extend([2] * len(train_texts))\n",
    "\n",
    "# Ajouter les données de validation avec le label 1\n",
    "combined_texts.extend(validation_texts)\n",
    "combined_labels.extend([2] * len(validation_texts))\n",
    "\n",
    "# Ajouter les données de test avec le label 2\n",
    "combined_texts.extend(test_texts)\n",
    "combined_labels.extend([2] * len(test_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "5b8ec32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  Label\n",
      "0  AS THEY'RE LEAVING <COMMA> CAN KASH PULL ZAHRA...      2\n",
      "1                              SIX TOMATOES <PERIOD>      2\n",
      "2  AND SOMETHING BROUGHT BACK RESTORED FROM THE R...      2\n",
      "3  TO HELP SCREEN READER USERS IN THE MIDST OF DI...      2\n",
      "4  FOR ALICE HAD READ SEVERAL NICE LITTLE STORIES...      2\n"
     ]
    }
   ],
   "source": [
    "audio_df = pd.DataFrame({\n",
    "    'Text': combined_texts,\n",
    "    'Label': combined_labels\n",
    "})\n",
    "\n",
    "# Affichage des premières lignes pour vérifier\n",
    "print(audio_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b2f3dead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AS THEY'RE LEAVING &lt;COMMA&gt; CAN KASH PULL ZAHRA...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SIX TOMATOES &lt;PERIOD&gt;</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AND SOMETHING BROUGHT BACK RESTORED FROM THE R...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TO HELP SCREEN READER USERS IN THE MIDST OF DI...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOR ALICE HAD READ SEVERAL NICE LITTLE STORIES...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NOT BEGGING &lt;PERIOD&gt;</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HE PULLS OUT</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A CAPACITY KNOWN AS NEUROPLASTICITY</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>IT SEEMED THE REPORTER HAD JUST BECOME A CHARA...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GENERAL HEALTH &lt;COMMA&gt; THE QUALITY OF LIFE &lt;PE...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Label\n",
       "0  AS THEY'RE LEAVING <COMMA> CAN KASH PULL ZAHRA...      2\n",
       "1                              SIX TOMATOES <PERIOD>      2\n",
       "2  AND SOMETHING BROUGHT BACK RESTORED FROM THE R...      2\n",
       "3  TO HELP SCREEN READER USERS IN THE MIDST OF DI...      2\n",
       "4  FOR ALICE HAD READ SEVERAL NICE LITTLE STORIES...      2\n",
       "5                               NOT BEGGING <PERIOD>      2\n",
       "6                                       HE PULLS OUT      2\n",
       "7                A CAPACITY KNOWN AS NEUROPLASTICITY      2\n",
       "8  IT SEEMED THE REPORTER HAD JUST BECOME A CHARA...      2\n",
       "9  GENERAL HEALTH <COMMA> THE QUALITY OF LIFE <PE...      2"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "7f4d9999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can cross training benefit groups like run...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Are there any particular physical benefits to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What percentage of the Earth's surface is cove...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does language translation technology impac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the most popular smartphone brand thes...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What role does collaboration and collective cr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What are the main types of logical fallacies, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What makes reality TV shows so addictive? As a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How do satellite images help with environmenta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Can mindful walking be used as a form of medit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Label\n",
       "0  How can cross training benefit groups like run...      1\n",
       "1  Are there any particular physical benefits to ...      1\n",
       "2  What percentage of the Earth's surface is cove...      1\n",
       "3  How does language translation technology impac...      1\n",
       "4  What is the most popular smartphone brand thes...      1\n",
       "5  What role does collaboration and collective cr...      1\n",
       "6  What are the main types of logical fallacies, ...      1\n",
       "7  What makes reality TV shows so addictive? As a...      1\n",
       "8  How do satellite images help with environmenta...      1\n",
       "9  Can mindful walking be used as a form of medit...      1"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "cff0eb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompts</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Polaroid photo of a middle-aged woman with fa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"glamor photo of a teenage girl with short cur...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"high fashion photo of a child with a cute exp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"large format photo of a elderly man with a be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"instant photo of a young man wearing a suit a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"paparazzi photo of a teenage girl with short ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"high fashion photo of a elderly man with a be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"candid photo of a child with a cute expressio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"paparazzi photo of a elderly man with a beard...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"Polaroid photo of a young man wearing a suit ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             prompts  Label\n",
       "0  \"Polaroid photo of a middle-aged woman with fa...      0\n",
       "1  \"glamor photo of a teenage girl with short cur...      0\n",
       "2  \"high fashion photo of a child with a cute exp...      0\n",
       "3  \"large format photo of a elderly man with a be...      0\n",
       "4  \"instant photo of a young man wearing a suit a...      0\n",
       "5  \"paparazzi photo of a teenage girl with short ...      0\n",
       "6  \"high fashion photo of a elderly man with a be...      0\n",
       "7  \"candid photo of a child with a cute expressio...      0\n",
       "8  \"paparazzi photo of a elderly man with a beard...      0\n",
       "9  \"Polaroid photo of a young man wearing a suit ...      0"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_PD.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "dc172a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_df = pd.concat([audio_df, llm_dataset,IMAGE_PD ], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "9c4ac5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.set_default_device(\"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "da57b116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Router(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "        super(Router, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.Dropout(0.1),\n",
    "            # Ajoutez ici des couches supplémentaires\n",
    "            \n",
    "            \n",
    "            nn.Linear(128, 64),  # Nouvelle couche dense\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.Dropout(0.1),\n",
    "            # Continuez à ajouter autant de couches que vous le souhaitez\n",
    "            nn.Linear(64, num_experts)\n",
    "            \n",
    "        )\n",
    "          \n",
    "    def forward(self, x):\n",
    "        expert_logits = self.network(x)\n",
    "        return F.log_softmax(expert_logits, dim=-1)\n",
    "\n",
    "# Initialisation et utilisation du routeur\n",
    "num_experts = 3\n",
    "input_dim = 2782\n",
    "router = Router(input_dim, num_experts).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "6e93743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        # Tokenize the text\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "        text,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',  # Ensures output is a PyTorch tensor\n",
    "        )\n",
    "        input_ids = inputs['input_ids'].squeeze(0)\n",
    "        attention_mask = inputs['attention_mask'].squeeze(0)\n",
    "        return input_ids, attention_mask, label\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "6c0ed6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True, padding=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "last_df_2 = last_df\n",
    "last_df_2.head(10)\n",
    "texts = last_df_2[\"Text\"]\n",
    "\n",
    "\n",
    "#texts = last_df['Text'].values\n",
    "labels = last_df['Label'].values\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    texts_padded = pad_sequence(texts, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_masks_padded = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "\n",
    "    labels = torch.tensor(labels)\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "\n",
    "text_dataset = TextDataset(texts, labels, tokenizer)\n",
    "text_dataloader = DataLoader(text_dataset, batch_size=32, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "754f8980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4055 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[260], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, data  \u001b[38;5;129;01min\u001b[39;00m text_dataloader:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(data)\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "Cell \u001b[1;32mIn[259], line 15\u001b[0m, in \u001b[0;36mcollate_batch\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollate_batch\u001b[39m(batch):\n\u001b[1;32m---> 15\u001b[0m     texts_padded \u001b[38;5;241m=\u001b[39m pad_sequence(texts, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding_value\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id)\n\u001b[0;32m     16\u001b[0m     attention_masks_padded \u001b[38;5;241m=\u001b[39m pad_sequence(attention_masks, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     18\u001b[0m     labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(labels)\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\nn\\utils\\rnn.py:400\u001b[0m, in \u001b[0;36mpad_sequence\u001b[1;34m(sequences, batch_first, padding_value)\u001b[0m\n\u001b[0;32m    396\u001b[0m         sequences \u001b[38;5;241m=\u001b[39m sequences\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    398\u001b[0m \u001b[38;5;66;03m# assuming trailing dimensions and type of all the Tensors\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;66;03m# in sequences are same and fetching those from sequences[0]\u001b[39;00m\n\u001b[1;32m--> 400\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mpad_sequence(sequences, batch_first, padding_value)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got str"
     ]
    }
   ],
   "source": [
    "for data, data  in text_dataloader:\n",
    "    print(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "3ae7ba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = Adam(router.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "epochs = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "11938436",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x2369 and 2782x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[253], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m data, target \u001b[38;5;241m=\u001b[39m  data\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 8\u001b[0m output \u001b[38;5;241m=\u001b[39m router(data)\n\u001b[0;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, target)\n\u001b[0;32m     10\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[236], line 30\u001b[0m, in \u001b[0;36mRouter.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 30\u001b[0m     expert_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork(x)\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlog_softmax(expert_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\utils\\_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[1;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x2369 and 2782x256)"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for data, target in text_dataloader:  # dataset doit être un DataLoader\n",
    "        \n",
    "        data, target =  data.float().to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = router(data)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c81d78e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-conda-env",
   "language": "python",
   "name": "my-conda-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
